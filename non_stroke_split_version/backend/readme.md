# Non-Stroke Split Version Backend
## QuickStart:
1. The steps for quick deployment are as follows:
Before starting, please complete the following steps:
- Download base_train_data and unzip it. Place all the images inside in the folder alg/style_transfer/zi2zi/base_train_data in the project directory. Download link is as follows: https://zjueducn-my.sharepoint.com/:u:/g/personal/21921307_zju_edu_cn/EYo6vXCYnodKtQQKSja4NlMBrcuRDKwBkJCFZbXV5xH4NA?e=UueuPd (Alternate link：https://gofile.me/73DMg/SDi5n3U4e)
- Based on existing open-source community solutions and fine-tuning guidelines, fine-tune the open-source Stable Diffusion pre-trained weights using batch-exported Chinese character images from the font. For specific fine-tuning guidelines, you can refer to: https://huggingface.co/docs/diffusers/training/text2image. Please save the trained model to the 'alg/style_transfer/stablediffusion/base_model' folder in the project directory after training is complete.
- Execute "accelerate config" to determine the GPU precision used by StableDiffusion Finetune.
 
2. Prepare Env：
```
conda create -n font_generator python=3.9
conda activate font_generator
pip install -r requirements.txt
```
3. modify config.py
4. run main app
```
python app.py
```

## Repository Description
1. The alg folder contains algorithm-related content, including inference for zi2zi and stablediffusion-related content. 
2. The data folder contains zips and upload_imgs, which are temporary directories for user-uploaded content. 
3. The result_data folder contains result files for each run, named according to the timestamp, and has the following structure: it contains relevant result files.
- 2023-08-15 18-21-43
    - style_transfer
        - sd_finetune
            - infer_result
            - logging
            - model
        - zi2zi_finetune
            - data
            - finetune
            - infer_data
            - train_result
4. Under normal circumstances, the system integrates training and display, providing a one-stop solution for users. Users upload compressed packages in the frontend, wait for the backend training to complete, and then perform vectorization operations directly on the frontend. However, if users have completed training and inference and only want to perform vectorization operations, they can follow the steps below:
Open the is_preview mode in config.py, and place the files generated by the training algorithm into the corresponding directory under result_data/preview. The specific steps are as follows: 1. For example, if you already have a series of images generated by the network inferencing after SD fine-tuning, you need to name them as "you_0.png", "you_1.png", etc. If there are only 2 images for each character, you need to change sd_infer_num to 2 in config.py, and modify the charset in config.py to the corresponding character set. For example, if there is only the character "you", charset='you'. Then place these images in the directory result_data/preview/style_transfer/sd_finetune/infer_result. In preview mode, no content needs to be placed in other directories. Once you open the frontend, you can start the vectorization process directly.

Example:
1) Please Download json from：https://zjueducn-my.sharepoint.com/:u:/g/personal/21921307_zju_edu_cn/EW451bpvwL1As2dAUTxoToUB4YwpQ87E7NvIwpiPcYsAjQ?e=8JBsHZ
(Alternative link：https://gofile.me/73DMg/ohfqgBR8Y )
2) Unzip the compressed package and place all json files in the directory result_data/preview/style_transfer/sd_finetune/infer_result
3) Change is_preview to True in config.py, and modify the charset to '廒霸镑煸埔潺帱赐幌阳胤璎猷缀缒' or any other desired character set. 
4) Then, go to the frontend directory and operate.


